---
layout: default
title: Projects
---
<div style="margin:0px 150px 0px 150px;">
	<h2>{{ page.title }}</h2>
	<p>
		These are some of the projects I've tried out for fun.
	</p>
	<ul class="posts" style="list-style-type:none;">
		<!--{% for post in site.posts %}-->
		<li>
			<h3>
				TrueSkill
			</h3>
			Neat application of applying a Bayesian approach to ML. I wanted to understand the algorithm Microsoft use to
			rank players in online matchmaking so I implemented the original TrueSkill
			<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2007/01/NIPS2006_0688.pdf">
				paper</a>.
			<br>
			<br>
			<i class="fa fa-pencil"></i>
			<a href="https://nush395.github.io/2020/06/06/true-skill.html" title="Understanding TrueSkill">
			Understanding TrueSkill
			</a> (Blog Post - approx 2 hours read)
			<br>
			On ranking, factor graphs, expectation propagation and how useful Gaussians are! One acid test for me
			to see if I've learnt something is to try explain it to someone else so this blog post is my attempt at
			that. The original paper is old in ML years but super interesting so please check it out!
			<br>
			<i class="fa fa-github"></i>
			<a href="https://github.com/Nush395/TrueSkill">
				Python implementation of TrueSkill
			</a>
			(Repo)
		</li>
		<br>
		<li>
			<h3>Network Pruning</h3>
			There is a lot of interesting literature on sparse networks. In particular I enjoyed this paper on
			<a href="https://arxiv.org/abs/1803.03635">the lottery ticket hypothesis</a> from ICLR 2019. In this project
			I have a go myself at conducting some experiments on network pruning using both weight and unit pruning.
			<br>
			<br>
			<i class="fa fa-github"></i>
			<a href="https://github.com/Nush395/pruning/blob/master/network_pruning.ipynb">Python pruning notebook</a>
			(Notebook)
			<br>
			The notebook can be opened in Colab so please have a go at running through yourself! I learnt several cool
			things in this project like how to stop weight updates on pruned weights and did a bit of exploration into
			taking advantage of sparsity in Tensorflow to speed up inference.
		</li>
		<br>
		<li>
		<h3>
			Analog RNN
		</h3>
		I'm always excited by seeing articles that tie together ML and Physics. This
		<a href="https://advances.sciencemag.org/content/5/12/eaay6946/tab-pdf">
			paper
		</a>
		formulates the wave equation using RNN update equations and shows how you could design a manifold with a non-linear
		wave speed across it to classify vowels by measuring the energy at points on the manifold!
		<br><br>
		<i class="fa fa-github"></i>
		<a href="https://github.com/Nush395/analog-rnn">
			Python implementation of Analog RNN
		</a>  (Repo WIP)
		</li>
			<!--{% endfor %}-->
	</ul>
</div>
