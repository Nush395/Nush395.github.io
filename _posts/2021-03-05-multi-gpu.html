---
layout: post
title: "Using multiple GPUs with Tensorflow"
date: 2021-05-03
---
<h2>
    Overview
</h2>
<p>
I recently investigated using multiple GPUs using <code>Tensorflow</code>. There's a bunch of
    <a href="https://www.tensorflow.org/guide/distributed_training">useful documentation</a> for doing
    distributed training with TF. The <code>tf.distribute.Strategy</code> API is extremely powerful already and
    seems to be getting better release on release. It has a lot of support for <code>Keras</code> models. It also works
    with custom training loops and general <code>Tensorflow</code> backed computation. As the docs
    <a href="https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_custom_training_loops">say</a>
    however this requires a bit more effort. The following is a method I found of using multiple GPUs without
    <code>tf.distribute.Strategy</code> that I found useful for simple cases that I thought I'd share. I'm going
    to illustrate with an example that takes advantage of data parallelism but the principle can be applied to
    model parallelism too. I'm going to assume some familiarity with terminology in what follows but if needed the
    docs above are very good at introducing ways GPU parallelism can be achieved.
</p>
<h2>
    Example - data parallelism
</h2>
<p>
    Say we have a function of <code>TF</code> operations that take in a batch of input data. As we are going to exploit data
    parallelism, we assume our function can work on each sample in the batch individually.
</p>
<pre>
    <code class="python">
        import tensorflow as tf

        def expensive_operation(input_data: tf.data.Dataset):
            """Beastly function that does a bunch of expensive operations on input."""
            ...

            return output_data
    </code></pre>
<p>
    We're currently exploiting
    a GPU to accelerate these operations but it's still running too slowly.
    If you have multiple GPU devices available on your host then out of the box <code>Tensorflow</code> will pick the lowest index one by default and all the others will be
    left alone.
</p>
<h2>
    GPU utilisation
</h2>
<p>
    The first thing we will want to do to check if our function is using that single GPU efficiently. Ideally we want
    GPU utilisation to be over 90% (this means over 90% of the cores on the GPU are doing something). If GPU utilisation
    isn't high then your operations aren't currently a good candidate for exploiting other GPUs as you're better off trying
    to use the single one more efficiently first. The simplest way
    to check this is to use <code>nvidia-smi</code>. Running the below will snapshot information about your GPUs every
    <code>num_secs</code> seconds and allow you to tell whether the utilisation is where it needs to be.
</p>
<pre>
    <code>
        nvidia-smi watch -n num_secs
    </code>
</pre>
If so,
    great, crack onto the next stage! If not there is some
    <a href="https://www.tensorflow.org/guide/gpu_performance_analysis">useful documentation</a>
    on how to increase that utilisation.
<h2>
    Using <code>tf.device</code>
</h2>
<h4>
    Get a list of available devices
</h4>
<p>
    <code>Tensorflow</code>
    <a href="https://www.tensorflow.org/api_docs/python/tf/device">provides</a>
    the <code>tf.device</code> context manager for manually pinning operations to a certain device.
    To get a list of what GPU devices you have available by name use:
</p>
<pre>
    <code>
        gpus = tf.config.list_physical_devices('gpu')
    </code>
</pre>
<h4>
    Pin to single device
</h4>
<p>
    We can now explicitly pin our operation to any GPU we want. For example let's imagine we have
    2 GPUs available and pin the above operations to the second one (indexed by 1).
</p>
<pre>
    <code>
        gpus = tf.config.list_physical_devices('gpu')
        with tf.device(gpus[1].name):
            expensive_operation(input_data)
    </code>
</pre>
<h4>
    Run on multiple devices (sequentially)
</h4>
<p>
    It's as easy as that, I really like how <code>Tensorflow</code> enables you device access so easily.
    Now we can use multiple GPUs at the same time!
</p>
<pre>
    <code>
        gpus = tf.config.list_physical_devices('gpu')
        num_gpus = len(gpus)
        batch_data = input_data.batch(num_gpus)
        for batch, gpu in zip(batch_data, gpus):
            with tf.device(gpu.name):
                expensive_operation(batch)
    </code>
</pre>
<h4>
    Run on multiple devices in parallel!
</h4>
<p>
    Hang on a sec. With <code>Tensorflow</code> running in eager execution mode (as it does by default). This
    will just run on each of the GPUs sequentially. There is a
    <a href="https://www.tensorflow.org/guide/gpu#manual_placement">section</a>
    in the <code>Tensorflow</code> docs that describes the above too.
    The trick here is to run in graph mode and then everything will run in parallel.
</p>
<pre class="python">
    <code>
        @tf.function
        def parallel_expensive_operation(input_data: tf.data.Dataset):
            gpus = tf.config.list_physical_devices('gpu')
            num_gpus = len(gpus)
            batch_data = input_data.batch(num_gpus)
            for batch, gpu in zip(batch_data, gpus):
                with tf.device(gpu.name):
                    expensive_operation(batch)
    </code>
</pre>
<p>
    On the first call of the <code>tf.function</code> we have to pay a cost to compile the graph but usually
    this shouldn't be too bad..
</p>
<h4>
    Capturing output
</h4>
<p>
    The last piece that's worth mentioning explicitly is about capturing output. As we're now working inside graph mode
    we can't simply append outputs to a list, as <code>Python</code> side effects are only executed
    on the first iteration of the graph (at compile time). One solution to this is to use a
    <code>tf.TensorArray</code> instead of a <code>Python</code> list to store our outputs.
    As a quick example..
</p>
<pre class="python">
    <code>
        @tf.function
        def parallel_expensive_operation(input_data: tf.data.Dataset):
            gpus = tf.config.list_physical_devices('gpu')
            num_gpus = len(gpus)

            results_container = tf.TensorArray(
                dtype=...,
                size=num_gpus,
                infer_shape=False,  # Required for different shape Tensors given by data shard.
            )
            batch_data = input_data.batch(num_gpus)
            for i, (batch, gpu) in enumerate(zip(batch_data, gpus)):
                with tf.device(gpu.name):
                    results_container = results_container.write(i, expensive_operation(batch))
    </code>
</pre>
<h2>
    That's it!
</h2>
<p>
    There a couple of things to be wary of when using multiple GPUs. For example
    any <code>tf.Variable</code>s are created on a specific device. On a multi-gpu host this will be the GPU with the
    lowest index unless specified otherwise. Depending on the data transfer speed between your GPU devices this can be
    quite important as it means that variables need to be read from one GPU device to another. If for example your data
    transfer speed is slow and a variable is read across devices many times we want to avoid this. I may write about
    that in a seperate blog post but hopefully this has been useful for now! If you're interested in doing
    distributed computations two more useful things to check out are
    <a href="https://github.com/google/jax">Jax</a>
    and
    <a href="https://ray.io">Ray</a>.
    Thanks for reading :)
</p>