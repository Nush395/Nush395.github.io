---
layout: post
title: "Understanding TrueSkill"
date: 2020-06-06
---
<h2>
    Overview
</h2>
<p>
    In this post I'd like to give an explanation of the TrueSkill algorithm created by Microsoft Research for ranking
    teams of players in games. My hope is that if can gather my thoughts down coherently enough to explain the algorithm to
    someone else then I'll finally have completed  my journey in understanding the TrueSkill algorithm.
    This post is an accompaniment to the open source
    <a href="https://github.com/Nush395/TrueSkill">Python implementation</a>
    I created where I replicated the
    <a href="https://www.microsoft.com/en-us/research/publication/trueskilltm-a-bayesian-skill-rating-system/">
        original paper.
    </a>
    Along with the original paper another really helpful resource I'd thoroughly encourage you to check out is this
    <a href="https://www.moserware.com/assets/computing-your-skill/The%20Math%20Behind%20TrueSkill.pdf">
        article
    </a>
    by Jeff Moser which explains the intuition and a lot of the detail behind the equations very well. That article
    saved me a lot of pain by explaining some of the complicated concepts and likely explains most of what I'm going
    to explain in this post. That said this is as much an exercise for me to check my understanding by explaining so
    let's crack on anyway!
</p>
<h2>
    An Intro to Ranking
</h2>
    <h4>
        An example
    </h4>
<p>
    This all started with me getting repeatedly thrashed at table football by the same person, let's call them Player A
    over and over again. You don't have to feel sorry for me because everyone in my office gets repeatedly thrashed by
    Player A at table football. Occasionally one of us will get lucky and beat them but it's fairly obvious that the
    person that wins the most games against everyone individually is the best, I don't need a fancy algorithm to tell
    me that. It does however leave a lot of questions unanswered. How good is Player A? If they go and play someone who is
    the best in another office who should I be rationally favouring? Who's the second best in my office and how far away
    from Player A are they? If I had a bunch of data points of matches between players and their outcomes I should be
    able to answer these questions.
</p>
    <h4>
        Skill
    </h4>
<p>
    That last question in particular points towards a way of answering these questions.
    If we imagine that each player has some <i>skill</i> attribute that represents how good they are then we could look
    at the relative difference between different players' skills to compare who is better but also how much better they
    are. It also makes sense to model this <i>skill</i> attribute with a probability distribution as in the real world
    we're never going to know for certain what the exact value is but we could have a good idea of what the expectation
    of that quantity is and how certain we are of our estimate. Awesome, so we know have a way of representing someone's
    goodness at the game. The next bit is figuring out how we could model who to rationally bet on and also how we could
    use that data of matches and game outcomes to better estimate a player's skill. To illustrate this in the next
    chapter let's build a toy model of a game involving two players using this idea that each player has some skill value
    and we expect a player with higher skill to win.
</p>
<h3>
    Toy model
</h3>
<p>
    We can construct a simple toy model of a game between two players as follows
    <ul>
        <li>
            Take Player 1 to have skill $w_1$ and  Player 2 to have skill $w_2$ ($w_i \in \mathbb{R}$).
        </li>
        <li>
            Compute the difference in the player skills: $$s = w_1 - w_2$$
        </li>
        <li>
            Add some Gaussian noise ($n \sim \mathcal{N}(0,\,\sigma^{2}$)) to account for the uncertainty in the match:
            $$t=s+n$$.
        </li>
        <li>
            We can say that the game outcome could be determined by $y=sign(t)$;
            $$y=1 \hspace{1em} \text{implies Player 1 won.}$$
            $$y=-1 \hspace{1em} \text{implies Player 2 won.}$$
        </li>
    </ul>
</p>
<h3>
    Bayesian treatment of the model
</h3>
<p>
    We can take a Bayesian treatment of this model by placing a prior probability over the
    initial skills and then use the observed game outcome to compute the posterior probability
    of the skills given the game outcome using Bayes' theorem. Let's go ahead and do that then...
    <ol>
        <li>
            Choosing a Gaussian distribution to represent the prior probabilities we have:
            $$w_i \sim \mathcal{N}(\mu_i,\,\sigma_i^{2})$$
        </li>
        <li>
            If we take the game noise to have standard deviation of 1, $n \sim \mathcal{N}(0,\,1)$ for simplicity then,
            the likelihood is given by $$p(y|w_1,w_2) = \Phi(y(w_1-w_2))$$
            <p>
            <button data-toggle="collapse" data-target="#likelihood-derivation">Click to see likelihood derivation</button>
            <div id="likelihood-derivation" class="collapse">
                $$t = w_1 - w_2 + n \Rightarrow p(t|w_1,w_2) = \mathcal{N}(w_1-w_2,\,1)$$
                As y is a binary variable let's work out each case:
                $$p(y=1|w_1,w_2) = p(t>0|w_1,w_2) = \Phi(w_1-w_2)$$
                $$p(y=-1|w_1,w_2) = p(t<0|w_1,w_2) = 1 - \Phi(w_1-w_2) = \Phi(-(w_1-w_2))$$
                $$\therefore p(y|w_1,w_2) = \Phi(y(w_1-w_2))$$
            </div>
            </p>
        </li>
        <li>
            Using Bayes' theorem we can now work out the posterior distribution of the skills given the game outcome:
            $$
            p(w_1,w_2|y) = \frac{\mathcal{N}(w_1;\mu_1,\,\sigma_1^{2})\mathcal{N}(w_2;\mu_2,\,\sigma_2^{2})\Phi(y(w_1-w_2))}
                                      {\Phi(\frac{y(\mu_1-\mu_2)}{\sqrt{1+\sigma_1^{2}+\sigma_2^{2}}})}
            $$
            <p>
            <button data-toggle="collapse" data-target="#posterior-derivation">Click to see posterior derivation</button>
            <div id="posterior-derivation" class="collapse">
                \begin{align}
                p(w_1,w_2|y) &= \frac{p(w_1)p(w_2)p(y|w_1,w_2)}{\iint p(w_1)p(w_2)p(y|w_1,w_2) \,dw_1\,dw_2}
                           \\  &= \frac{\mathcal{N}(w_1;\mu_1,\,\sigma_1^{2})\mathcal{N}(w_2;\mu_2,\,\sigma_2^{2})\Phi(y(w_1-w_2))}
                                     {\iint \mathcal{N}(w_1;\mu_1,\,\sigma_1^{2})\mathcal{N}(w_2;\mu_2,\,\sigma_2^{2})\Phi(y(w_1-w_2))\,dw_1\,dw_2}
                           \\ &= \frac{\mathcal{N}(w_1;\mu_1,\,\sigma_1^{2})\mathcal{N}(w_2;\mu_2,\,\sigma_2^{2})\Phi(y(w_1-w_2))}
                                      {\Phi(\frac{y(\mu_1-\mu_2)}{\sqrt{1+\sigma_1^{2}+\sigma_2^{2}}})}
                \end{align}
            </div>
            </p>
        </li>
    </ol>
</p>
<h3>
    Problems with the model
</h3>
<p>
    Even for this simple model we've arrived at something pretty ugly looking for our posterior. Our skill variables
    $w_1$ and $w_2$ have become correlated and it's not possible to factorise this distribution. It's not really obvious
    how we answer any of our above questions easily with a posterior like this. We'd also ideally like to be able to use
    this posterior as the prior for these players in their next game but it's not possible to marginalise out either of
    the player's skills. To proceed we'll have to take another approach but before we do, it will be useful to take a
    break to introduce the topic of Factor graphs, another concept used in the TrueSkill paper that I will talk about in
    the next section. If you're familiar with factor graphs then feel free to skip ahead!
</p>
<h2>
    Factor graphs and message passing
</h2>
<p>
    One of the concepts that the TrueSkill paper introduced to me was that of factor graphs.
    <a href="https://en.wikipedia.org/wiki/Factor_graph">Factor graphs</a>
    are a type of probabilistic graphical model that allow us to represent the product factor structure of a function as
    a bipartite graph of factors and their variables.
    We can take advantage of this structure to enable efficient computation using something called the Sum-Product
    algorithm. To get an intuition for why using the factor structure of a function is
    useful consider the calculation:
    $$ab+ac$$
    To get the answer we have to do 3 operations.
    <ol>
        <li>
            $a \times b$
        </li>
        <li>
            $a \times c$
        </li>
        <li>
            Sum the two terms.
        </li>
    </ol>
    We could do this more efficiently if we notice that we can factorise the above expression to get:
    $$a(b+c)$$
    Now we only have to do 2 operations.
    <ol>
        <li>
            $b+c$
        </li>
        <li>
            Multiply the answer by a.
        </li>
    </ol>
    Coming back to the the example at hand we will want to be able to get marginal distributions from our
    joint distribution of player skills and game outcomes which will involve summing over the joint distribution.
    By turning sums of products into products of sums we can get similar improvements in our marginalisation. To avoid
    making this post to long I'll simply encourage you to read about the theory behind factor graphs and the sum-product
    algorithm on your own and simply brush past to applying the concept to our toy model.
</p>
<h2>
    True Skill Factor Graph
</h2>
<p>
    We can represent the toy model we described above with a factor graph that looks like this:
<div class="row">
    <div class="col-sm-4">
        <img src="{{site.baseurl}}/assets/img/factor_graph_two_player.png" height="400"/>
    </div>
    <div class="col-sm-8">
        <p>
            Let's now again use this model and the sum product message passing algorithm to compute the
            updated skill marginal for Player 1 in a game where Player 1 wins:
        </p>
        <ol>
            <li>
                First we pass messages from the prior factors to the variable $w_i$. This is simply:
                $$m_{prior \to w_i}=\mathcal{N}(w_i;\mu_i,\,\sigma_i^{2})$$
            </li>
            <li>
                Next we compute the message from the skill variables $w_i$ to the game factor $g$, again this
                is straightforward as messages from variable to a factor is simply the product of all incoming messages
                to the variable except the incoming message from that factor. There's only one other incoming
                message to $w_i$, the message that just arrived from the prior factor, so we simply pass that on.
                $$m_{w_i \to f}=\mathcal{N}(w_i;\mu_i,\,\sigma_i^{2})$$
            </li>
            <li>
                We know that Player 1 won so we must have observed a the game outcome of $y=1$. Therefore we also pass
                a message from the game factor $k$ "up" to variable k.
                $$m_{g \to t} = \delta(1-sign(t))$$
            </li>
            <li>
                Again messages from variables to a factor are simply the product of all messages incoming to the variable
                except from that factor, so we simply pass the delta function along.
                $$m_{t \to f} = \delta(1-sign(t))$$
            </li>
            <li>
                The message from the factor to $w_1$ is the product of the incoming messages from $t$ and $w_2$ and the
                factor summing out all the variables except $w_1$.
                $$m_{f \to w_1} = \Phi(\frac{\mu_2-w_1}{\sigma_2^{2}+1})$$

                <button data-toggle="collapse" data-target="#cumgauss-derivation">Click to see derivation.</button>
                <div id="cumgauss-derivation" class="collapse">
                    \begin{align}
                        m_{f \to w_1} &= \iint \! \delta(1-sign(t))\mathcal{N}(t;w_1-w_2,1)\mathcal{N}(w_2;\mu_2,\,\sigma_2^{2}) \,  \mathrm{d}t\mathrm{d}w_2
                                      &= \int \! \delta(1-sign(t)) \int \mathcal{N}(t;w_1-w_2,1)\mathcal{N}(w_2;\mu_2,\sigma_2^{2} \, \mathrm{d}w_2\mathrm{d}t
                                      &= \int \! \delta(1-sign(t))\mathcal{N}(t;w_1-\mu_2,\sigma_2^{2}+1) \, \mathrm{d}t
                                      &= \int_{0}^{\infty} \! \mathcal{N}(t;w_1-\mu_2,\sigma_2^{2}+1) \, \mathrm{d}t
                                      &= \Phi(\frac{\mu_2-w_1}{\sigma_2^{2}+1})
                    \end{align}
                    Going from the second to third line I've used the result for the marginal distribution where the conditional
                    and other marginal distribution are Gaussian. (See Chapter 2.3.3 of Bishops book.)
                </div>
            </li>

        </ol>
    </div>
</div>



